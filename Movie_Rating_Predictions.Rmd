---
title: "Movie Rating Predictions"
author: "Nick Aristidou"
date: "15/11/2021"
output: pdf_document
extra_dependencies: ["float"]
sansfont: Arial
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', fig.width=15, knitr.table.format = "latex",fig.pos = "!H", out.extra = "",out.width = "90%",options(digits = 3))
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(lubridate)
library(knitr)
library(kableExtra)
library(data.table)
library(dplyr)
library(rafalib)
```

# Movie Recommender

## Abstract
This research project investigates the creation of a recommendation system for movies, where Naive Bayes models and a Matrix Factorization model are employed. To compare the models the Residual Mean Square Error is calculated. The best Naive Bayes model achieved a RMSE of 0.84 on the test set and a RMSE of 0.825 on the validation set, whilst the matrix factorization model achieved a RMSE of 0.79 on the test set.


## Introduction
The aim of this of this research capstone is to generate a movie recommendation system, using the [MovieLens](https://movielens.org/) dataset. The data used herein is produced by a research lab from the University of Minnesota, GroupLens, whom have compiled a collection of millions of movie ratings. Specifically, for this project the 10M version is utilized. The project will explore the data and then will aim to create a model that given a user will predict the rating for a movie. To evaluate model performance the metric Root Mean Squared Error (RMSE) will be employed, which is determined by the square root of the residual sum of squares from comparing predictions $\hat{y}$ with observed outcomes $y$ for each user $u$ and each item (movie) $i$, S. Glen (2020) :

$\sqrt{\frac{1}{N}\sum_{u,i=1}^{N} (y_{u,i} - \hat{y}_{u,i})}$


## Data loading and partitioning into a train, test and validation split.
To create a model that can predict ratings, the data set needs to broken down into a train, test and validation split. Machine learning requires this split so that a model can be fitted on a sample of the data, the train set, has a subset of data to provide an unbiased evaluation of a model fit and tune hyperparameters, the validation set. Finally there also needs to be a sample of data to provide an unbiased evaluation of the final model, the test set. 

Special consideration to the creation of these sets is required for a recommendation system as the model needs to learn user and movie features, therefore each set must contain the userId and movieId for each user and movie in the original data. The following code, loads the data creates the first partition into the train and validation sets, whilst ensuring each set contains an occurrence of every userId and movieId:

```{r loading_movie_data, echo=TRUE, message=FALSE}
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```


Further partitioning of the train data to create the final data sample, the test set. 
```{r test_train_split, echo=TRUE, include=TRUE, message=FALSE}
set.seed(1)
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
train <- edx[-test_index,]
temp <- edx[test_index,]

# Make sure userId and movieId in  test set are also in train set
test <- temp %>% 
  semi_join(train, by = "movieId") %>%
  semi_join(train, by = "userId")

# Add rows removed from testset back into train set
removed <- anti_join(temp, test)
train <- rbind(train, removed)

rm(test_index, temp, removed)
```

The final sets of data are of the following form: 
    1.    Train set - `r train %>% ncol()` columns and `r train %>% nrow()` rows.
    2.    Validation set - `r validation %>% ncol()` columns and `r validation %>% nrow()` rows.
    3.    Test set - `r test %>% ncol()` columns and `r test %>% nrow()` rows.

## Exploratory Data Analysis and Visualisation
To begin the following table shows the data is comprised of the columns userId, movieId, rating, timestamp, title and genres and depicts the type of information stored within. It is noted that the data contains `r length(edx$userId %>% unique())` unique user ids and `r length(edx$movieId %>% unique())` unique movie ids. Furthermore the ratings range from `r edx$rating %>% min()` up to `r edx$rating %>% max()`, with increments of 0.5.

```{r glimpse, echo=FALSE,include=TRUE}
train %>% glimpse()
```

A useful way to visualize the task of a recommendation system is to generate a rating matrix grid, which transforms the data into users as rows and movies as columns. Each cell is then occupied by the rating that a given user has reviewed a given movie, whilst unseen/unrated movies remain blank. The figure below takes a subset of 100 users to generate this rating matrix grid and shows the issue of sparsity. Most users will only have rated a small number of movies out of the entire movie library. 

```{r rating_matrix, echo=FALSE}
users <- sample(unique(train$userId), 100)
rafalib::mypar()
train %>% filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% select(sample(ncol(.), 100)) %>% 
  as.matrix() %>% t(.) %>%
  image(1:100, 1:100,. , xlab="Movies", ylab="Users")
abline(h=0:100+0.5, v=0:100+0.5, col = "grey")
title("Ratings")
```

To further explore the data the following sections will examine effects and trends in the data associated with the movies, users, time and the genre of the movie.

### Exploring Movie Effect
The top 10 most rated movies in the data set are:
```{r top_10_movies, echo=FALSE}
edx %>%
  group_by(title) %>%
  summarize(count = n()) %>%
  arrange(-count) %>%
  top_n(10, count) %>%
  ggplot(aes(count, reorder(title, count))) +
  geom_bar(color = "black", fill = "blue", stat = "identity") +
  xlab("Count") +
  ylab(NULL) +
  theme_bw()
```

The distribution of movie ratings is as follows and shows that a rating of 4.0 is the most frequently given value. 
```{r rating_distribution, echo=FALSE}
edx %>%
  ggplot(aes(rating, y = ..prop..)) +
  geom_bar(color = "black", fill = "blue") +
  labs(x = "Ratings", y = "Relative Frequency") +
  scale_x_continuous(breaks = c(0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5)) +
  theme_bw()
```

The number of ratings for a movie follows an approximately normal distribution, as shown below.
```{r movie_rating, echo=FALSE}
edx %>% 
  group_by(movieId) %>%
  summarize(n = n()) %>%
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black", fill = "blue", alpha = 0.75) + 
  scale_x_log10() + 
  ggtitle("Grouping Movies by Number of Ratings") +
  xlab("Number of Ratings") + ylab("Number of Movies")
```
The top 10 most rated films and their average rating 
```{r most_rated, echo=FALSE}
edx %>% 
  group_by(movieId, title) %>%
  summarize(No_Ratings = n(), Average = mean(rating),
            .groups = 'drop') %>%
  arrange(desc(No_Ratings)) %>%
  slice_head(n = 10) %>%
  kbl(.,booktabs = T) %>%
  kable_styling(latex_options = c("striped","hold_position"))
```
The average rating distribution for movies is also approximately normal, with the average movie rating at `r edx$rating %>% mean()`. 
```{r ave_movie_rating, echo=FALSE}
edx %>% group_by(movieId) %>% 
  summarize(Average = mean(rating)) %>%
  ggplot(aes(x = Average)) + 
  geom_histogram(bins = 30, color = "black", fill = "blue", alpha = 0.75) + 
  ggtitle("Grouping Movies by Average Rating") +
  xlab("Average Rating") + ylab("Number of Movies")
```

From the above visuals it is noted that the movieId will be critical in generating rating predictions as each movie will effect the outcome. For example a movie that is always consistently rated highly should therefore obtain a higher rating from the model. However, the visuals also highlighted that the range in the number of ratings a movie will receive varies, with some movies receiving many and others receiving very few. A range of factors could be at play here, a movie could be very new in the data and thus has not been watched/rated by many users yet and contributes to a well known issue in recommendation systems known as the "cold-start" problem. Further more a film may not be attractive to the majority of audiences and covers a niche topic. It is likely these types of films will also receive higher ratings as they are more likely watched by users that will enjoy the content. The range in the number of ratings for a given movie is highly likely to generate a source of error in the models deployed and will contribute to larger values of RMSE.

### User Effects Investigation
The figure below shows the distribution of the number of ratings given by users and exhibits a right skew.
```{r user_distribution, echo=FALSE}
edx %>%
  group_by(userId) %>% 
  summarize(n = n()) %>%
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black", fill = "darkgreen", alpha = 0.75) + 
  scale_x_log10() +
  ggtitle("Grouping Users by Number of Ratings") +
  xlab("Number of Ratings") + ylab("Number of Users")
```
The table below shows the Top 10 users by rating and their average movie rating.
```{r top_10_users, echo=FALSE}
edx %>% 
  group_by(userId) %>%
  summarize(No_Ratings = n(), Average = mean(rating)) %>%
  arrange(desc(No_Ratings)) %>%
  top_n(n = 10, No_Ratings) %>% kbl(.,booktabs = T) %>%   kable_styling(latex_options = c("striped","hold_position"))
```
The average user rating distribution is displayed below and also shows a normal distribution.
```{r ave_user_rating, echo=FALSE}
edx %>%
  group_by(userId) %>% 
  summarize(Average = mean(rating)) %>%
  ggplot(aes(x = Average)) + 
  geom_histogram(bins = 30, color = "black", fill = "darkgreen", alpha = 0.75) + 
  ggtitle("Grouping Users by Average Rating") +
  xlab("Average Rating") + ylab("Number of Users")
```

### Time Effect
The timestamp field represents the number of seconds past midnight from 1 January 1, 1970. The following figure, shows average rating as function of the year a rating was given. As there is only one rating in 1995, which is a 5, this value can be treated as an outlier and if excluded it is noted that the avg rating by year is consistent and unlikely to prove useful as a predictor for rating. 
```{r time_effect,echo=FALSE, message=FALSE}
edx %>% 
  mutate(Year = year(as_datetime(timestamp))) %>%
  group_by(Year) %>%
  summarize(avg_rating = mean(rating)) %>%
  ggplot(aes(Year, avg_rating)) +
  geom_point() +
  geom_smooth() +
  ggtitle("Time Effect") +
  xlab("Date") + ylab("Rating")
```
Likewise, further investigation shows that neither month nor the day of the rating considerable impact the average rating given. Both of these features are shown in the figures below, respectively.
```{r month_of_rating, echo=FALSE, message=FALSE}
edx %>% 
  mutate(Month = month(as_datetime(timestamp))) %>%
  group_by(Month) %>%
  summarize(avg_rating = mean(rating)) %>%
  ggplot(aes(Month, avg_rating)) +
  geom_point() +
  geom_smooth() +
  ggtitle("Time Effect") +
  xlab("Date") + ylab("Rating")
```

```{r day_of_rating, echo=FALSE,message=FALSE}
edx %>% 
  mutate(Day = day(as_datetime(timestamp))) %>%
  group_by(Day) %>%
  summarize(avg_rating = mean(rating)) %>%
  ggplot(aes(Day, avg_rating)) +
  geom_point() +
  geom_smooth() +
  ggtitle("Time Effect") +
  xlab("Date") + ylab("Rating")
```


### Genre Effect
In contrast to the time effect, it is noted that the genre of the movie can impact the rating of the movie. This is show in the figure below, where the ratings as a function genre are displayed. This may indicate that genre could be a useful feature to include to help improve accuracy of the prediction model. 
```{r genre_effects, echo=FALSE}
edx %>% 
  group_by(genres) %>%
  summarize(n = n(), avg = mean(rating), stdev = sd(rating), se = sd(rating)/sqrt(n())) %>%
  filter(n > 100000) %>%
  ggplot(aes(x = genres, y = avg, ymin = avg - 2*se, ymax = avg + 2*se)) + 
  geom_point() + 
  geom_errorbar(width=0.75, colour="black", alpha=0.75, size=0.25) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_hline(yintercept=mean(edx$rating),color="blue",linetype='dashed') +
  ggtitle("Genre Effect") +
  xlab("Genres") + ylab("Rating")
```

## Model Building

### Initial Prediction
```{r average, echo=FALSE}
mu <- mean(train$rating)
```
Employing the simple approximation that a movie will be rated with the average movie rating, `r mu`, generates a model that defaults to this value for every prediction. This model can be represented by the equation $\hat{Y}_{u,i} = \mu + \epsilon_{u,i}$, where $\epsilon_{u,i}$ is th error distribution. This model generates the following RMSE evaluation:

```{r result_1, echo=FALSE}
naive_rmse <- RMSE(test$rating,mu)
result <- tibble(Method = "Naive Bayes", RMSE = naive_rmse)
result %>% kbl(.,booktabs = T) %>%   kable_styling(latex_options = c("striped","hold_position"))
```


### Add movie effect term
To enhance the model, the effect of a movie can be included as this was noted to earlier to show a strong relationship to the rating. This movie effect $\hat{b}_i$ can be calculated as the mean of the difference between the observed rating $y$ and the mean $\mu$ and takes the form:

$\hat{b}_i = \frac{1}{N}\sum_{i=1}^{N}{(y_i - \hat{\mu})}$

With the overal model represented by:
$\hat{Y}_{u,i} = \mu + b_i + \epsilon_{u,i}$

```{r movie_effect, echo =FALSE}
movie_avg <-
  train %>%
  group_by(movieId) %>%
  summarise(b_i = mean(rating - mu),
            .groups='drop')

model_2 <- 
  mu + test %>%
  left_join(movie_avg,by="movieId") %>%
  pull(b_i)

movies_rmse <- RMSE(test$rating,model_2)

result <- bind_rows(result, 
                    tibble(
                      Method = "Movie Effect",
                      RMSE = movies_rmse)
)

result %>% kbl(.,booktabs = T) %>%   kable_styling(latex_options = c("striped","hold_position"))
```


### Add user effect
Additionally from the previous EDA, it was shown that a given user had an impact on ratings, where a unique user has there own preferences and tastes and will rate a movie accordingly. As an example one user may consistently given ratings of 4 or 5, whilst another may given lower scores of 1 to 2. To account for this effect the model can be expanded to include the user term $\hat{b}_u$. 

$\hat{b}_u = \frac{1}{N}\sum_{u,i=1}^{N}{(y_{u,i} - \hat{b}_i - \hat{\mu})}$

This generates the model of the form:

$\hat{Y}_{u,i} = \mu + b_i + b_u + \epsilon_{u,i}$

```{r movie_user_effect, echo=FALSE}
user_avg <- train %>% 
  left_join(movie_avg, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

model_3 <- test %>% 
  left_join(movie_avg, by='movieId') %>%
  left_join(user_avg, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

movie_user_rmse <- RMSE(test$rating, model_3)
result <- bind_rows(result, 
                    tibble(
                      Method = "Movie + User Effects",
                      RMSE = movie_user_rmse)
)

result %>% kbl(.,booktabs = T) %>%   kable_styling(latex_options = c("striped","hold_position"))

```

### Add genres effect
Expanding the model to also include a genre effect as per the above logic leads to a model of the form:

$\hat{Y}_{u,i} = \mu + b_i + b_u + b_g + \epsilon_{u,i}$

```{r movie_user_genre_effect, echo = FALSE}
genre_avg <- train %>%
   left_join(movie_avg, by='movieId') %>%
   left_join(user_avg, by='userId') %>%
   group_by(genres) %>%
   summarize(b_g = mean(rating - mu - b_i - b_u))

model_4 <- validation %>%
   left_join(movie_avg, by='movieId') %>%
   left_join(user_avg, by='userId') %>%
   left_join(genre_avg, by='genres') %>%
   mutate(pred = mu + b_i + b_u + b_g) %>%
   pull(pred)

movie_user_genre_rmse <- RMSE(validation$rating, model_4)
result <- bind_rows(result, 
                    tibble(
                      Method = "Movie + User + Genres Effects",
                      RMSE = movie_user_genre_rmse)
)

result %>% kbl(.,booktabs = T) %>%   kable_styling(latex_options = c("striped","hold_position"))
```

It is worth noting this has had little to no impact on the model, which may seem suprising since there appeared to be correlation between genre and rating. The lack of any significant impact on RMSE, may arise as this factor may be captured in the user term. A user is likely to have a favored genre and will rate thesse higher than films in other genres. 


### Add time
Here for research purposes only, the year of the review is also considered but noted that this will likely have no effect as per the fact that the timestamp had very little to no correlation to ratings. 

Including a year effect modifies the model to the following:

$\hat{Y}_{u,i} = \mu + b_i + b_u + b_g + b_{yr} + \epsilon_{u,i}$


```{r year_effect, echo=FALSE}
train <- train %>% mutate(Year = year(as_datetime(timestamp)))

test <- test %>% mutate(Year = year(as_datetime(timestamp)))

year_avg <- train %>% 
  left_join(movie_avg, by='movieId') %>%
  left_join(user_avg, by='userId') %>%
  left_join(genre_avg, by='genres') %>%
  group_by(Year) %>%
  summarize(b_yr = mean(rating - mu - b_i - b_u - b_g))

model_5 <- test %>%
  left_join(movie_avg, by='movieId') %>%
  left_join(user_avg, by='userId') %>%
  left_join(genre_avg, by='genres') %>%
  left_join(year_avg, by='Year') %>%
  mutate(pred = mu + b_i + b_u + b_g + b_yr) %>%
  pull(pred)

movie_user_genre_year_rmse <- RMSE(test$rating, model_5)
result <- bind_rows(result, 
                    tibble(
                      Method = "Movie + User + Genres + Year Effects",
                      RMSE = movie_user_genre_year_rmse)
)

result %>% kbl(.,booktabs = T) %>%   kable_styling(latex_options = c("striped","hold_position"))
```

## Validate Model
To validate the model, the validation data set is employed and predicted ratings are produced. These predications are then compared to the actual ratings contained in this dataset and the RMSE is calculated to determine the success of the model. (A lower RMSE is a better model)
```{r validation, echo = FALSE}
edx <- edx %>% mutate(Year = year(as_datetime(timestamp)))

validation <- validation %>% mutate(Year = year(as_datetime(timestamp)))

movie_avg <-
  edx %>%
  group_by(movieId) %>%
  summarise(b_i = mean(rating - mu),
            .groups='drop')

user_avg <- edx %>% 
  left_join(movie_avg, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

genre_avg <- edx %>%
   left_join(movie_avg, by='movieId') %>%
   left_join(user_avg, by='userId') %>%
   group_by(genres) %>%
   summarize(b_g = mean(rating - mu - b_i - b_u))

year_avg <- edx %>% 
  left_join(movie_avg, by='movieId') %>%
  left_join(user_avg, by='userId') %>%
  left_join(genre_avg, by='genres') %>%
  group_by(Year) %>%
  summarize(b_yr = mean(rating - mu - b_i - b_u - b_g))

model_5 <- validation %>%
  left_join(movie_avg, by='movieId') %>%
  left_join(user_avg, by='userId') %>%
  left_join(genre_avg, by='genres') %>%
  left_join(year_avg, by='Year') %>%
  mutate(pred = mu + b_i + b_u + b_g + b_yr) %>%
  pull(pred)

movie_user_genre_year_rmse <- RMSE(validation$rating, model_5)
result <- bind_rows(result, 
                    tibble(
                      Method = "Model Validation",
                      RMSE = movie_user_genre_year_rmse)
)

result %>% kbl(.,booktabs = T) %>%   kable_styling(latex_options = c("striped","hold_position"))
```


## Model Tuning - Regularization
As previously described some users have only rated very few films, and some movies have very few ratings leading to small sample sizes for these instances. Statistically speaking this introduces a larger estimated error. The user and movie effect terms, $b_u and b_i$, can strongly influence predictions made by the model and error in these will cause the model performance to deteriorate. To combat this a technique known as regularization can be employed, R. Irizarry (2022). This introduces a tuning parameter, $\lambda$, that can penalize distortions to these terms in essence by penalizing small sample sizes. For example the addition of $\lambda$ modifies the movie effect term $b_i$ to the following:

$\hat{b}_i = \frac{1}{n_i + \lambda}\sum_{u=1}^{n_i}{(y_{u,i} - \hat{\mu})}$

The following set of movies shown highlight this issue where some films have a low number of ratings and their movie effect term is significant.
```{r low_movie_reviews, echo = FALSE}
movie_titles <- edx %>% 
     select(movieId, title) %>%
     distinct() 

edx %>%
     count(movieId) %>% 
     left_join(movie_avg, by="movieId") %>%
     left_join(movie_titles, by="movieId") %>%
     arrange(desc(b_i)) %>% 
     select(title, b_i, n) %>% 
     slice(1:10) %>% kbl(.,booktabs = T) %>%   kable_styling(latex_options = c("striped","hold_position"))
```
To optimize the value of $\lambda$, multiple simulations a run against multiple values of $\lambda$:
```{r tuning_1, echo=FALSE}
lambda <- 1
movie_reg <- edx %>% 
     group_by(movieId) %>% 
     summarize(b_i = sum(rating - mu)/(n()+lambda), n_i = n()) 

tibble(original = movie_avg$b_i, 
           regularlized = movie_reg$b_i, 
           n = movie_reg$n_i) %>%
     ggplot(aes(original, regularlized, size=sqrt(n))) + 
     geom_point(shape=1, alpha=0.5)
```

```{r tuning_2, echo=TRUE}
ls_focused <- seq(0, 2, 0.1)

reg_rmses_focused <- sapply(ls_focused, function(l) {

   # Calculate the regularized avges for movie, user, genre, and time
   
   movie_avgs_reg <- edx %>%
      group_by(movieId) %>%
      summarize(b_i = sum(rating - mu) / (n() + l))
   
   user_avgs_reg <- edx %>%
      left_join(movie_avgs_reg, by='movieId') %>%
      group_by(userId) %>%
      summarize(b_u = sum(rating - mu - b_i) / (n() + l))
   
    genre_avgs_reg <- edx %>%
      left_join(movie_avgs_reg, by='movieId') %>%
      left_join(user_avgs_reg, by='userId') %>%
      group_by(genres) %>%
      summarize(b_g = sum(rating - mu - b_i - b_u) / (n() + l))
    
    year_avgs_reg <- edx %>% 
      left_join(movie_avgs_reg, by='movieId') %>%
      left_join(user_avgs_reg, by='userId') %>%
      left_join(genre_avgs_reg, by='genres') %>%
      group_by(Year) %>%
      summarize(b_yr = sum(rating - mu - b_i - b_u - b_g) / (n() + l))
  
   #predict the movie ratings on edx set
   pred_movie_user_genre_year_reg <- edx %>%
      left_join(movie_avgs_reg, by='movieId') %>%
      left_join(user_avgs_reg, by='userId') %>%
      left_join(genre_avgs_reg, by='genres') %>%
      left_join(year_avgs_reg, by='Year') %>%
      mutate(pred = mu + b_i + b_u + b_g + b_yr) %>%
      pull(pred)
   
   #calculate RMSE
   RMSE(edx$rating, pred_movie_user_genre_year_reg)
})

# pick the final lambda
plot(ls_focused, reg_rmses_focused)
```

```{r tuning_3, echo=FALSE}
l_final <- ls_focused[which.min(reg_rmses_focused)]

reg_rmse_validation <- sapply(l_final, function(l) {

   # Calculate the regularized avg for movie, user, genre, and time
   movie_avgs_reg <- validation %>%
      group_by(movieId) %>%
      summarize(b_i = sum(rating - mu) / (n() + l))
   
   user_avgs_reg <- validation %>%
      left_join(movie_avgs_reg, by='movieId') %>%
      group_by(userId) %>%
      summarize(b_u = sum(rating - mu - b_i) / (n() + l))
   
    genre_avgs_reg <- validation %>%
      left_join(movie_avgs_reg, by='movieId') %>%
      left_join(user_avgs_reg, by='userId') %>%
      group_by(genres) %>%
      summarize(b_g = sum(rating - mu - b_i - b_u) / (n() + l))
    
    year_avgs_reg <- validation %>% 
      left_join(movie_avgs_reg, by='movieId') %>%
      left_join(user_avgs_reg, by='userId') %>%
      left_join(genre_avgs_reg, by='genres') %>%
      group_by(Year) %>%
      summarize(b_yr = sum(rating - mu - b_i - b_u - b_g) / (n() + l))
  
   #predict the movie ratings on validation set
   pred_movie_user_genre_year_reg <- validation %>%
      left_join(movie_avgs_reg, by='movieId') %>%
      left_join(user_avgs_reg, by='userId') %>%
      left_join(genre_avgs_reg, by='genres') %>%
      left_join(year_avgs_reg, by='Year') %>%
      mutate(pred = mu + b_i + b_u + b_g + b_yr) %>%
      .$pred
   
   #calculate RMSE
   RMSE(validation$rating, pred_movie_user_genre_year_reg)
})
```

```{r tuning_4, echo=FALSE}
result <- 
  result %>%
  add_row("Method" = "Regularized Model", "RMSE" = reg_rmse_validation)

result %>% kbl(.,booktabs = T) %>%   kable_styling(latex_options = c("striped","hold_position"))
```

## Model Evaluation - Test Set

```{r model_eval, echo=FALSE}
l_final <- ls_focused[which.min(reg_rmses_focused)]

reg_rmse_test <- sapply(l_final, function(l) {

   # Calculate the regularized avg for movie, user, genre, and time
   movie_avgs_reg <- test %>%
      group_by(movieId) %>%
      summarize(b_i = sum(rating - mu) / (n() + l))
   
   user_avgs_reg <- test %>%
      left_join(movie_avgs_reg, by='movieId') %>%
      group_by(userId) %>%
      summarize(b_u = sum(rating - mu - b_i) / (n() + l))
   
    genre_avgs_reg <- test %>%
      left_join(movie_avgs_reg, by='movieId') %>%
      left_join(user_avgs_reg, by='userId') %>%
      group_by(genres) %>%
      summarize(b_g = sum(rating - mu - b_i - b_u) / (n() + l))
    
    year_avgs_reg <- test %>% 
      left_join(movie_avgs_reg, by='movieId') %>%
      left_join(user_avgs_reg, by='userId') %>%
      left_join(genre_avgs_reg, by='genres') %>%
      group_by(Year) %>%
      summarize(b_yr = sum(rating - mu - b_i - b_u - b_g) / (n() + l))
  
   #predict the movie ratings on test set
   pred_movie_user_genre_year_reg <- test %>%
      left_join(movie_avgs_reg, by='movieId') %>%
      left_join(user_avgs_reg, by='userId') %>%
      left_join(genre_avgs_reg, by='genres') %>%
      left_join(year_avgs_reg, by='Year') %>%
      mutate(pred = mu + b_i + b_u + b_g + b_yr) %>%
      .$pred
   
   #calculate RMSE
   RMSE(test$rating, pred_movie_user_genre_year_reg)
})

result <- 
  result %>%
  add_row("Method" = "Model Evaluation", "RMSE" = reg_rmse_test)

result %>% kbl(.,booktabs = T) %>%   kable_styling(latex_options = c("striped","hold_position"))

```


## Matrix Factorisation
One of the most widely used tools in recommendation systems is matrix factorisation, and was popularized in industrial applications after emerging as the winner from the Netflix prize. This method aims to fill in the rating matrix grid shown earlier by representing this large sparse matrix by the product two lower dimension matrices. The R library [recosystem](https://www.r-bloggers.com/2016/07/recosystem-recommender-system-using-parallel-matrix-factorization/) provides a method to decompose the matrix into these lower dimensions and then allow predictions to be made, Yixuan (2016).

To demonstrate matrix factorization, the following code breaks down the steps the recosystem library is performing. It is worth noting there are many methods employed to solve the dimensionality reduction of the matrix and the example below is for illustrative purposes and may not replicate the method employed by the recosystem library, J. Brownlee (2018). This example is also only run on a sample of 10 users and 10 movies only as this operation is computational expensive on large datasets and to achieve the results that the library delivers requires the use of C programming languages to handle memory.

```{r mf_func, echo=FALSE}
matrix_fact <-
  function(R,P,Q,K, steps=5000,alpha=0.002,beta=0.02){
    #R rating matrix
    #P user features matrix
    #Q item (movie) features matrix
    #K latent features
    #steps = iterations
    #alpha = learning rate
    #beta = regularization parameter
    Q = t(Q)
    for (step in 1:steps){
      for (i in 1:nrow(R)){
        for (j in 1:ncol(R)){
          if (R[i,j] >0) {

            eij <- R[i,j] - (P[i,]%*%Q[,j])

            for (k in 1:K){
              P[i,k] = P[i,k] + alpha * (2 * eij * Q[k,j] - beta * P[i,k])
              Q[k,j] = Q[k,j] + alpha * (2 * eij * P[i,k] - beta * Q[k,j])
            }
          }
        }
      }

    e = 0

    for (i in 1:nrow(R)){

      for (j in 1:ncol(R)){

        if (R[i,j] > 0) {

          e = e + (R[i,j]-(P[i,]%*%Q[,j]))^2

          for (k in 1:K){
            e = e + (beta/2) * ((P[i,k])^2+ (Q[k,j])^2)
          }

        }

      }

    }

    if (e < 0.001){
      break
    }

    }

    return(list(
          P=P,
          Q = t(Q))
    )

  }
```

```{r mf_demo}
#Generate random sample of 10 users
users <- sample(unique(train$userId), 10)

# R is the rating matrix grid and 
R<-
  train %>% filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  spread(movieId, rating) %>% select(sample(ncol(.), 10)) %>% 
  as.matrix() %>% replace_na(0)
R
```

Decompose matrix R into two lower dimension matricies P and Q:
```{r mf_demo2}
N <- nrow(R)
M <- ncol(R)
#latent features:
K = 3

P <- matrix(rexp(N*K), N)
Q <- matrix(rexp(M*K), M)

output <- matrix_fact(R,P,Q,K)
output
```

Combine P and Q to generate filled in matrix
```{r mf_demo3}
mf_example <- output$P %*% t(output$Q)
mf_example
```

Only values that were previously blank are needed for the recommendation system to truly work as these are what would be used to decided if a user would like the movie/item.

```{r mf_1, echo=FALSE,message=FALSE, results='hide'}
library(recosystem)
#Convert train set into rating matrix grid
train_set = data_memory(user_index = train$userId,
                        item_index = train$movieId,
                        rating = train$rating, index1 = TRUE)

r = Reco()
opts_tune = r$tune(train_set,                                   
                   opts = list(dim = c(10, 20, 30),        
                               costp_l2 = c(0.01, 0.1),         
                               costq_l2 = c(0.01, 0.1),         
                               costp_l1 = 0,                    
                               costq_l1 = 0,                    
                               lrate    = c(0.01, 0.1),         
                               nthread  = 4,                    
                               niter    = 10,                   
                               verbose  = TRUE))                
r$train(train_set, opts = c(opts_tune$min,                      
                            niter = 100, nthread = 4))

```

Generate predictions against the test data set:
```{r evaluate_mf}
test_set= data_memory(test$userId, test$movieId, index1 = TRUE)
pred = r$predict(test_set, out_memory())
mf_rmse <- RMSE(pred,test$rating)

result <- 
  result %>%
  add_row("Method" = "Matrix Factorization", "RMSE" = mf_rmse)

result %>% kbl(.,booktabs = T) %>% kable_styling(latex_options = c("striped","hold_position"))

```

Heatmap to show how the rating matrix has now been populated with predictions:
```{r mf_2, echo=FALSE, fig.width=5,fig.height=5}
user = 1:20
movie = 1:20
pred = expand.grid(user = user, movie = movie)
test_set = data_memory(pred$user, pred$movie, index1 = TRUE)
pred$rating = r$predict(test_set, out_memory())

library(ggplot2)
ggplot(pred, aes(x = movie, y = user, fill = rating)) +
    geom_tile() +
    scale_fill_gradient("Rating", low = "#d6e685", high = "#1e6823") +
    xlab("Movie ID") + ylab("User ID") +
    coord_fixed() +
    theme_bw(base_size = 22)
```

## Conclusion
In summary, it was noted by exploration of the MovieLens data that movie and the user had the greatest impact on the variability of the rating from the mean value. The initial approach to build a recommendation system was built upon deploying the average movie rating. This yielded and RMSE of 1.06, which fails to account for biases exhibited by users and biases fro movies. The basis model was expanded to include these terms in addition to the genre of the movie and the time of the rating. These additions improved the model performance as noted by the improved RMSE of ca. 0.865. It is noted that the majority of this improvement derives from the user and movie effect terms, however for exploratory purposes the genre and time terms were still included. 

To enhance the model further, regularization was applied to help reduce the error introduced from small samples that occur when a movie has very few ratings or where a user has rated very few movies. The tuning parameter $\lambda$ was optimized against a simulation aimed at minimizing RMSE. With the optimal tuning parameter selected the model was deployed on the test set and a final RMSE value of 0.84 was achieved. This model reported a RMSE of 0.825 on the validation dataset.

Expanding beyond this, the project explored the use of matrix factorization to fill in the sparse rating matrix grid. Using this technique, a RMSE of 0.79 can be achieved.


## Additional Comments
There are many challenges that affect recommendation systems, one of which has been the issue of the cold-start problem. Here recommenders struggle to deal with new users or new movies. The models employed herein would fail to predict a rating for these, and alternative methods of handling this issue need to be explored. 

Furthermore, recommendation systems face ethical challenges and content based challenges were companies need to be careful of the recommendations made. For example the content of the item may not be suitable for the age of the user and filtering of their respective item libraries may be required. Diversity of recommendations is also another issue, were if a recommendation system keeps recommending very similar types of movies a user my never see new types of content. Here a potential solution is to ensure that in a top n recommendation a handful of the recommendations are diverse and are items unlikely to be seen. In a similar vein the preferences of a user may change with time, something they were interested in a year ago may not interest them today. Again solutions to this may involve the application of filtering the user catalogs to only look at recent recommendations and drop historical ones. 

It is also worth noting that user/item databases can get very large very quickly and the concepts and application of Big Data may need to be applied to run and generate models. The computing power of one laptop/computer may be insufficient to compute the matrix factorization of a dataset of the likes of Amazon or Netflix, C. Kasula (2020). 


## References
1. GroupLens, MovieLens. Available at: https://grouplens.org/datasets/movielens/ (Accessed on the 10 November 2021)
2. Glen Stephanie. RMSE: Root Mean Square Error. Available at: https://www.statisticshowto.com/probability-and-statistics/regression-analysis/rmse-root-mean-square-error/ (Accessed on the 10 November 2021)
3. Rafael A. Irizarry. (2022). Introduction to Data Science: Data Analysis and Prediction Algorithms with R. CRC Press.
4. Yixuan. recosystem: Recommender System Using Parallel Matrix Factorization. (2016). Available at: https://www.r-bloggers.com/2016/07/recosystem-recommender-system-using-parallel-matrix-factorization/ (Accessed on the 28 November 2021)
5. J. Brownlee. A Gentle Introduction to Matrix Factorization for Machine Learning. 2018. Avaliable at: https://machinelearningmastery.com/introduction-to-matrix-decompositions-for-machine-learning/ (Accessed on the 28 November 2021)
6. C. Kasula. Netflix Recommender System — A Big Data Case Study. Available at: https://towardsdatascience.com/netflix-recommender-system-a-big-data-case-study-19cfa6d56ff5 (Accessed on 3 January 2022)
